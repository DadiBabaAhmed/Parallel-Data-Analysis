FROM continuumio/miniconda3

# Install minimal OS utilities needed for Spark download and process listing
RUN apt-get update && apt-get install -y \
    wget \
    procps \
    openjdk-17-jre \
    && rm -rf /var/lib/apt/lists/*

# Copy environment spec early so it can be cached separately from the app code
COPY environment.yml /tmp/environment.yml

# Create a conda environment for the app with the pinned packages from environment.yml
RUN conda env create -f /tmp/environment.yml -n pda && conda clean -afy

# Ensure the conda env's bin directory is first on PATH
ENV PATH=/opt/conda/envs/pda/bin:$PATH

# Install Spark (kept after conda so spark binaries are available in the image)
ENV SPARK_VERSION=3.5.0
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH

RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} \
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Create working directory and copy project files
WORKDIR /app
COPY . /app

# Create output directories
RUN mkdir -p output/general output/general/graphs output/statistics output/failures

# Expose ports
EXPOSE 7077 8080 4040

# Set environment variables
ENV SPARK_MASTER_HOST=spark-master
ENV SPARK_MASTER_PORT=7077
ENV SPARK_MASTER_WEBUI_PORT=8080
ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

CMD ["/bin/bash","-c","/opt/spark/sbin/start-master.sh && tail -f /opt/spark/logs/*"]
